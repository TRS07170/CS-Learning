{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“Student_Yelp_Review_Sentiment_Classification.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiM6gYg0nhkY"
      },
      "source": [
        "<font color=\"#de3023\"><h1><b>MAKE A COPY OF THIS NOTEBOOK SO YOUR EDITS ARE SAVED</b></h1></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svLhCiU3Evm6"
      },
      "source": [
        "# Introduction to Yelp Review Sentiment Classification\n",
        "\n",
        "In this project, we will build a classifier that can predict a user's rating of a given restaurant from their review. This is an example of **sentiment analysis**: being able to quantify an individual's opinion about a particular topic merely from the words they use. \n",
        "\n",
        "**Discuss:** Can you think of other ways companies might use sentiment analysis?\n",
        "\n",
        "In this notebook, we'll:\n",
        "\n",
        "\n",
        "*   Explore the Yelp review dataset\n",
        "*   Preprocess and vectorize our text data for NLP\n",
        "*   Train a sentiment analysis classifier with logistic regression\n",
        "*   (Optional) Explore and improve our model\n",
        "*   (Optional, Advanced) Train a model with word embeddings\n",
        "*   (Optional, Challenge) Use word embeddings to calculate similarity and analogies\n",
        "\n",
        "Let's dive in!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZgELVS8I6kc"
      },
      "source": [
        "![Example of a Yelp review](https://wordstream-files-prod.s3.amazonaws.com/s3fs-public/styles/simple_image/public/images/yelp-reviews-filtered.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "2jS5ThMCEvnC",
        "outputId": "7a2ad9cb-3d1a-4e0b-c212-e13ed787066d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Import our libraries (this may take a minute or two)\n",
        "import pandas as pd   # Great for tables (google spreadsheets, microsoft excel, csv). \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "import wordcloud\n",
        "import os # Good for navigating your computer's files \n",
        "import sys\n",
        "pd.options.mode.chained_assignment = None #suppress warnings\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "!python -m spacy download en_core_web_md\n",
        "import en_core_web_md\n",
        "text_to_nlp = en_core_web_md.load()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.10.8)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-py3-none-any.whl size=98051302 sha256=06db17dc47ff14dc8c4153e8814fb48371e184c06837a7da40f8b338c2da9aea\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-452jhf9g/wheels/69/c5/b8/4f1c029d89238734311b3269762ab2ee325a42da2ce8edb997\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsF8tioZLicU",
        "cellView": "form",
        "outputId": "4caa9814-5439-4fe6-fa45-e2893cd5029d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Import our data\n",
        "\n",
        "# import gdown\n",
        "#gdown.download('https://drive.google.com/uc?id=1u0tnEF2Q1a7H_gUEH-ZB3ATx02w8dF4p', 'yelp_final.csv', True)\n",
        "data_file  = 'yelp_final.csv'\n",
        "\n",
        "!wget https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%203%20-%20NLP/yelp_final.csv\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-21 17:30:29--  https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%203%20-%20NLP/yelp_final.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.145.128, 209.85.146.128, 209.85.147.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.145.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 760976 (743K) [text/csv]\n",
            "Saving to: ‘yelp_final.csv’\n",
            "\n",
            "\ryelp_final.csv        0%[                    ]       0  --.-KB/s               \ryelp_final.csv      100%[===================>] 743.14K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-12-21 17:30:29 (45.2 MB/s) - ‘yelp_final.csv’ saved [760976/760976]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ267zCBOjet"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BLs_2JkEvnw"
      },
      "source": [
        "First we read in the file containing the reviews and take a look at the data available to us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dZ_lymcN_K9",
        "outputId": "7500debc-5c6e-4a5c-b3e1-9ed62bc8e363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# read our data in using 'pd.read_csv('file')'\n",
        "yelp_full = pd.read_csv(data_file)\n",
        "yelp_full.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-42a17128-ed0e-42e6-8fd4-ae9bfac4e55a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "      <th>user_id</th>\n",
              "      <th>cool</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
              "      <td>5</td>\n",
              "      <td>My wife took me here on my birthday for breakf...</td>\n",
              "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
              "      <td>5</td>\n",
              "      <td>I have no idea why some people give bad review...</td>\n",
              "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
              "      <td>5</td>\n",
              "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
              "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
              "      <td>5</td>\n",
              "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
              "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>zp713qNhx8d9KCJJnrw1xA</td>\n",
              "      <td>5</td>\n",
              "      <td>Drop what you're doing and drive here. After I...</td>\n",
              "      <td>wFweIWhv2fREZV_dYkz_1g</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42a17128-ed0e-42e6-8fd4-ae9bfac4e55a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-42a17128-ed0e-42e6-8fd4-ae9bfac4e55a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-42a17128-ed0e-42e6-8fd4-ae9bfac4e55a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              business_id  stars  ... useful funny\n",
              "0  9yKzy9PApeiPPOUJEtnvkg      5  ...      5     0\n",
              "1  ZRJwVLyzEJq1VAihDhYiow      5  ...      0     0\n",
              "2  _1QQZuf4zZOyFCvXc0o6Vg      5  ...      2     0\n",
              "3  6ozycU1RpktNG2-1BroVtw      5  ...      0     0\n",
              "4  zp713qNhx8d9KCJJnrw1xA      5  ...      7     4\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjL5FrSLEvoP"
      },
      "source": [
        "**Discuss:**\n",
        "- Which column gives us our output: the user's sentiment? \n",
        "- Which column gives us our input: the review?\n",
        "- Why aren't businesses' and users' real names included? (You'll notice they're replaced with random strings through [hashing](https://medium.com/tech-tales/what-is-hashing-6edba0ebfa67)). Why aren't the real names included?\n",
        "\n",
        "Let's keep only the two columns we need:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB1yKcUtcpg9",
        "outputId": "4b1634ce-5973-4425-b7d2-c3706b526751",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "needed_columns = ['stars', 'text'] #YOUR CODE HERE: fill in the columns\n",
        "yelp = yelp_full[needed_columns]\n",
        "yelp.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1bb65688-5aa8-4317-871a-156d79396b50\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>My wife took me here on my birthday for breakf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>I have no idea why some people give bad review...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Drop what you're doing and drive here. After I...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1bb65688-5aa8-4317-871a-156d79396b50')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1bb65688-5aa8-4317-871a-156d79396b50 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1bb65688-5aa8-4317-871a-156d79396b50');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   stars                                               text\n",
              "0      5  My wife took me here on my birthday for breakf...\n",
              "1      5  I have no idea why some people give bad review...\n",
              "2      5  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...\n",
              "3      5  General Manager Scott Petello is a good egg!!!...\n",
              "4      5  Drop what you're doing and drive here. After I..."
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MExj8roOEvog"
      },
      "source": [
        "The text column is the one we are primarily focused with. Let's take a look at a few of these reviews to better understand our problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la3rUPKgEvoi",
        "cellView": "form",
        "outputId": "10ee4058-b599-433e-d879-de1057e2c666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Check out the text in differently rated reviews\n",
        "num_stars =  1#@param {type:\"integer\"}\n",
        "\n",
        "for t in yelp[yelp['stars'] == num_stars]['text'].head(20).values:\n",
        "    print (t) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "U can go there n check the car out. If u wanna buy 1 there? That's wrong move! If u even want a car service from there? U made a biggest mistake of ur life!! I had 1 time asked my girlfriend to take my car there for an oil service, guess what? They ripped my girlfriend off by lying how bad my car is now. If without fixing the problem. Might bring some serious accident. Then she did what they said. 4 brand new tires, timing belt, 4 new brake pads. U know why's the worst? All of those above I had just changed 2 months before!!! What a trashy dealer is that? People, better off go somewhere!\n",
            "Disgusting!  Had a Groupon so my daughter and I tried it out.  Very outdated and gaudy 80's style interior made me feel like I was in an episode of Sopranos.  The food itself was pretty bad.  We ordered pretty simple dishes but they just had no flavor at all!  After trying it out I'm positive all the good reviews on here are employees or owners creating them.\n",
            "I've eaten here many times, but none as bad as last night.\n",
            "Service was excellent, and highly attentive.\n",
            "Food, absolutely horrible.\n",
            "\n",
            "My expectation was they would serve a steak on par with their seafood. After all, they were charging 39 bucks for a ribeye. \n",
            "What I was hoping for was a 1- 1-1/2' thick steak, cooked Pittsburgh style as I had ordered. \n",
            "What I got a a 3/4 in thick piece of meat that was mostly fat, gristle, and in no way resembled Pittsburgh Style. \n",
            "Salad, similar to something you could get at Chick Filet\n",
            "Veggies, blah.\n",
            "Bread basket, ample, but day old, and if not, it certainly wasn't fresh. \n",
            "\n",
            "In addition to bad food, we were crammed into a small room where we were nuts to butts with 6 other tables, listening to conversations ranging from someone's recent bout with pinkeye, and another couple who elected to speak entirely in French, until the waiter showed up, then it was like they turned off the French switch and suddenly began speaking English. \n",
            "\n",
            "I've had it with this place.\n",
            "If I'm going to pay 150 bucks for dinner, it'll be at Mortons, or Maestro where the steaks are 1-1/2 in thick, cooked to perfection, and half of it doesnt wind up on the plate as fat and gristle\n",
            "I have always been a fan of Burlington's deals, however I will not be shopping at this one again. I went to return a belt... pretty simple. Instead, I stood in the customer service line for 15 minutes thanks to an employee of Burlington buying/putting things on layaway. It took three other staff to help her out. There were no words said to me except \"Hold on\". I was pissed. When the lady was finally done, the employee at the service asked \"what do you want?\" Serious help is needed there!\n",
            " If you work in the area of \"customer service\" I think you should have some.\n",
            "Another night meeting friends here.  I have to laugh.  Waited another 20 minutes for my beer to be refilled at the bar.  A girl even took my empty without even asking if I wanted a refill.  A new brunette girl that I don't recognize left the bar and sat down with her guy friends on the customer side AT 9:25 ON A FRIDAY NIGHT.  Another bartender had to ask her to come back and work.  Management....  Pull your head out of your ass!  Sad to watch....  I need to talk my friends into another place!\n",
            "Not busy at all but took nearly 45 min to get our meal.  Ordered the trout and was shocked to see lots and lots of bones. Hmmmmm. Well asked the waitress about it and she said \"they try the best they can\"  hmmmmmm isn't this a \"fish\" restaurant? \n",
            "They comped the trout but still not sure I would go back.\n",
            "Yikes, reading other reviews I realize my bad experience wasn't unique. As a server I make a very laid back customer. I like pretty much everything I eat and don't require a lot of attention from the waiter. \n",
            "\n",
            "La Piccola Cucina would benefit from just one extra person in the front of the house. Our guy, though adorable and friendly, was too busy to refill our drinks and to remember to bring our appetizer (though charged us for it). The ahi tuna (highly recommended over the other fish options he said) was so overcooked it was the color and consistency of chicken. Like other reviewers mentioned, he was frantic and made that clear to every customer. At one point I even saw him in the kitchen cooking - they need another person! \n",
            "\n",
            "I left super stressed out from the experience, which is very, very unusual for me. You can either have bad service or bad food, but not both.\n",
            "This is my first year participating in Arizona to sell clothing, just wandering how busy its goona be in there ??? little worried, can any one reply me ??\n",
            "really, I can't believe this place has received such high reviews from people.\n",
            "\n",
            "my lady and i walked in, and were greeted rather rudely by a pretentious bitch at the front with a monotone \"name please?\", instead of a warm, friendly, french welcome to this rather charming looking place. we didn't have a reservation, which from the looks of how dead empty the place was, didn't seem like a problem. until the hostess whisked through her reservation list and explained she'd try to \"fit us in\". it was 6pm, the place had one other couple dining, and we were informed that the next party was arriving at 7:30. we weren't really looking to hang out. it would have be awfully lovely if yelpers would have informed of the need to MAKE RESERVATIONS before going in here, because if you don't, you're automatically a piece of shit according to the staff. sorry, i don't plan very much ahead. next time i will, and it won't be here.\n",
            "\n",
            "anyways, after debating for a minute or two with herself, the hostess decided we could \"squeeze in\" and sat us right on top of the other couple inside the restaurant. she informed us that we could cork our own wine, for their low low price of 9 fucking dollars. in words only office space could adequately describe,\"coup des tartes, what is it exactly that you do?\". \n",
            "\n",
            "the food was good, nothing spectacular for $25+ a plate. we got a salad with apples and nuts and bleu cheese, which was served warm. now this is the first warm salad i've had all of my life, and by no means am i some sort of wordly food expert, but i believe salad should be cold and crisp. not soggy and warm. yuck. the cordon bleu was tasty and filling, however, the wobbly table was an extreme annoyance when cutting through the chicken, but it was solved (with no thanks to the staff) by placing a jack-in-the-box gift card under the leg. i will admit, the mashed potatoes are some of the creamiest and best i've ever had. my lady got the pork tenderloin, which had a strange peppery-sweet pairing which i didn't particularly care for. we were broke for desert, so unfortunately we didn't get to try any tarts. you would think for a place that promotes itself as being so poise would get some fancier menus, other than a $.15 piece of cardstock copied in bulk at kinkos.\n",
            "\n",
            "this place was rather disappointing. i honestly thought it would be way more comfy and welcoming inside but it really wasn't. it presents itself as being so high class and french and oh-la-la wee wee, but when it comes down to it, it's still located in a tiny shack of a house at 16th street and highland. maybe if it were located in the biltmore i'd give it a bit more cred.\n",
            "I was really excited about this event, maybe my expectations were too high, because I was really underwhelmed.  Very little shade, ran out of water, by the time I got there at 1pm (other obligations) 20% of the vendors were out of food, and by the time I ate my way through some, everyone else was already out.   60 bucks for that?  I don't think so.  \n",
            "\n",
            "And when I mentioned it to \"the person in charge\", he was patronizing, invalidating and kept telling me how happy the chefs were.  Well, I'm your consumer, big guy.  \n",
            "\n",
            "Sorry, Devoured, wish it could have been better. \n",
            "\n",
            "What food I did try, was pretty good.\n",
            "The beach paradise of Ixtapa-Zihuatenejo awaited us....unfortunately, so did a connecting flight in ARIZONA of all places...\n",
            "\n",
            "Long story short....1/3 of our luggage did not arrive at our final destination, various articles of our clothing are still missing as well as my cell phone charger...not to mention security forcing us to open sealed containers of my son's baby food and milk for litmus tests...\n",
            "\n",
            "I looked on the bright side....at least we didn't get deported =)\n",
            "See the huge sign outside that says $2.50? Yeah, that's for shirts only. I was caught off guard when I brought in my skirt and got nailed for about $3.79. As I sadly handed over my $5 bill, the lady at the counter said a lot of people had told her they felt betrayed by the sign outside. No kidding. I had originally been heading down the street to Regal on 7th Ave.--who does a fine job in half the pickup time--for the \"advertised\" fake price here. Trying to make the best of it,  I told the lady at least I'd learned they had an onsite alteration service, and maybe that was worth a buck. UH OH. Picked up my skirt a few weeks later with some Jersey Shore-type chick at the counter, and was told I owed another $3.79. I flat-out refused to pay twice. She screamed at me, chased me to my car, wrote down my license plate and said the police would be visiting. Ni-i-i-ice.\n",
            "Unless you are a regular or look like your wallet is fat don't expect the best service.  Entrees cost about 20-38 bucks and Anti pasta runs between 5-20 desserts are all 8 bucks unless you get aged balsamic vinegar.  They also have a big cheese selection. \n",
            "\n",
            "I made reservations for this restaurant and they sat us by the back door.  The waiter never once explained anything on the menu or suggested a wine to go with our meal.  He didn't even tell us about the cheese menu except to check on the one we wanted.  The other waiter for a table near ours went out of his way to explain the menu and go as far as to tell his tables how to eat the cheese ( it comes with jelly's and such) .  \n",
            "\n",
            "My meal was good I had the duck.  She had the salmon which tasted very fishy.  Not fresh at all.  The wine was 40 bucks for a carafe ( maybe if we would have got a 100 dollar bottle we would have gotten better service)  The Dessert was good it was a rich chocolate cake with nuts and chocolate sauce. Nothing too special.  \n",
            "\n",
            "Overall the experience made the meal not worth the 115 bucks in these times not to mention they overcharged my girls card for the drinks at the bar.  If we dont want to give 25% don't charge it plain and simple.  \n",
            "\n",
            "Just a warning reservations and customer service mean nothing.  We mentioned our complaints to the waiter and manager.  We received an apology but how bout comp our dessert or something.  Hyatt Gainey staff is also rude for the most part.  \n",
            "\n",
            "Two thumbs way down.\n",
            "Take your money elsewhere, unless you've got kids.  I really try to like this place.  A family member signed me up for the discount card, so I've been going more often, but I just don't love it.  It's simply ok, but the prices are outrageous.  And the sounds and animatronics are a huge distraction from the so-so food.  The cocktails are alright, but, again, the price is not right.  The ony thing fun about the place for an adult is the gift shop and the light-up cocktail glasses (which cost extra.)  I've seen a lot of happy families in here though, so I bet it's better if you have little ones to bring along.\n",
            "My friend kept telling me how good their lunches are... I tried it, but it didn't do anything for me.  The sandwich tasted like something you can get at Safeway, but twice the price.  For being called a Purveyor of Fine Foods, I expected the food to taste like not just any ordinary sandwich. \n",
            "\n",
            "What I had:\n",
            "Ham, cheese with portobello mushrooms on a panini with 2 sides of pasta, one was parmesan and the other was macaroni. Maybe it was the panini bread. I think the parmesan pasta was made there, but I know for sure the macaroni was not made there, it tasted like something I had at a picnic which was in a carton. It wasn't anything to brag home about. \n",
            "\n",
            "I found out the head chef made my sandwich... keep him in the kitchen and away from the customers, no personality whatsoever. I asked for his suggestion, and his reaction was \"anythings good.\" \n",
            "\n",
            "Normally, when I go to AJ's, I go for their desserts. Their desserts are fabulous and you can get individual servings at a reasonable price.  I can take an assortment home and have them for later. \n",
            "\n",
            "Our company does use them for their catering and I've had many compliments on their dishes, but pass on the lunches.\n",
            "Absolutely horrendous.  This post office will lose your mail (repeatedly), laugh about it to your face, lie about it, blow it off, make it seem like it's your fault and make you wish you had sent it UPS.\n",
            "Other than the really great happy hour prices, its hit or miss with this place. More often a miss. :(\n",
            "\n",
            "The food is less than average, the drinks NOT strong ( at least they are inexpensive) , but the service is truly hit or miss.\n",
            "\n",
            "I'll pass.\n",
            "Been here for many years but have seen a recent decline in quality service. Felt scolded by the bartender today at lunch. Felt disrespected as a customer. Service sucks stay away. \"bad service = bad review\"\n",
            "This place is not there anymore.\n",
            "Rarely do I give a 1 star rating but after four attempts in four months to get my fax line and internet rolling I can honestly say they have earned this.  After finally showing up for a scheduled appointment the tech told us our internet was fine but we disagree because we are getting the same speed we had with Qwest in Cave Creek (SLOW)  and they are charging us $64 a month for what they call the upgraded 20mbs.  Furthermore, I tried to send out a couple faxes after he left and the line is not responding.  Im going to drive down to the store tomorrow and demand that they remove the fees for service I have not received and just get efax.  As I noted in the first review we were excited to upgrade to faster internet so that makes this experience even worse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GZ6NN4yEvos"
      },
      "source": [
        "**Discuss:**\n",
        "\n",
        "\n",
        "*   What words are often used in highly rated reviews?\n",
        "*   What words are often used in low-rated reviews?\n",
        "*   Do you notice any interesting exceptions? (For example, \"The seating and ambience were impressive, but the food served to us was not\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQHod14KEvoz"
      },
      "source": [
        "#### Word Clouds\n",
        "\n",
        "Another way to take a look at the most prominent words in any given star rating is through the use of word clouds. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHY5IhnKEvo8"
      },
      "source": [
        "Edit the value in the cell below to see the word cloud for each star rating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FtMnf1zLEvo_"
      },
      "source": [
        "#@title Word cloud for differently rated reviews\n",
        "num_stars =  1#@param {type:\"integer\"}\n",
        "this_star_text = ''\n",
        "for t in yelp[yelp['stars'] == num_stars]['text'].values: # form field cell\n",
        "    this_star_text += t + ' '\n",
        "    \n",
        "wordcloud = WordCloud()    \n",
        "wordcloud.generate_from_text(this_star_text)\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn4v3upxEvpL"
      },
      "source": [
        "**What are the differences between the reviews that have 1, 2, 3, 4, and 5 stars?**\n",
        "\n",
        "Any surprising similarities? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpU7nrWTEvov"
      },
      "source": [
        "### Exercise: Rules for Sentiment Analysis\n",
        "\n",
        "Can you think of any combinations of words, or rules, that would indicate if a particular review is **positive** or **negative**? Note them down below:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "dxuZyKOKy6Cc"
      },
      "source": [
        "#@title Rules\n",
        "rule_1 = \"good\" #@param {type:\"string\"}\n",
        "rule_2 = \"bad\" #@param {type:\"string\"}\n",
        "rule_3 = \"disappointing\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4PQg8FhEvow"
      },
      "source": [
        "**Discuss: will rules like this work well?**\n",
        "\n",
        "It might not be enough to just see **whether** particular words are used! We also should look at **how much** they're used: the *number* of particular words might give us information about the user's opinion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dArbYofKN206"
      },
      "source": [
        "# Preparing Our Data for Machine Learning\n",
        "\n",
        "Of course, it's much more efficient to use machine learning to analyze our text than try to create rules by hand! \n",
        "\n",
        "We'll need to prepare our data to use logistic regression. First, let's prepare our output column:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bwasn11BfD4"
      },
      "source": [
        "### Exercise: Preparing to Classify\n",
        "We're going to try to predict the sentiment - **positive** or **negative** - based on a review's text. \n",
        "\n",
        "In order to reduce our problem to a **binary classification** (two classes) problem, we will:\n",
        "\n",
        " - label 4 and 5 star reviews as 'good'\n",
        " - label 1, 2, 3 star reviews as 'bad'\n",
        "\n",
        "Please complete the function below and run it to create a new `is_good_review` column!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck4iX6PITzHS",
        "outputId": "c2c62331-860a-4b9f-bdee-fae5374e19fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "def is_good_review(num_stars):\n",
        "    if num_stars >= 4: ### YOUR CODE HERE\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Change the stars column to either be 'good' or 'bad'.\n",
        "yelp['is_good_review'] = yelp['stars'].apply(is_good_review)\n",
        "yelp.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-326c608f-ec9f-47af-a6de-e9cedce348a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "      <th>is_good_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>1</td>\n",
              "      <td>If it wasn't for the bad food I would go here ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>1</td>\n",
              "      <td>This was absolutely horrible. I got the suprem...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>1</td>\n",
              "      <td>Yeah...notsomuch.\\n\\nSprinkles is sooooooooo n...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>1</td>\n",
              "      <td>I (like others) am at a loss as to why this pl...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1</td>\n",
              "      <td>When I was in highschool various clubs used to...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-326c608f-ec9f-47af-a6de-e9cedce348a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-326c608f-ec9f-47af-a6de-e9cedce348a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-326c608f-ec9f-47af-a6de-e9cedce348a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     stars                                               text  is_good_review\n",
              "995      1  If it wasn't for the bad food I would go here ...           False\n",
              "996      1  This was absolutely horrible. I got the suprem...           False\n",
              "997      1  Yeah...notsomuch.\\n\\nSprinkles is sooooooooo n...           False\n",
              "998      1  I (like others) am at a loss as to why this pl...           False\n",
              "999      1  When I was in highschool various clubs used to...           False"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8raBKzTEvpM"
      },
      "source": [
        "## Text Preprocessing: A Preview\n",
        "\n",
        "Now, the trickier part: preparing our text input.\n",
        "\n",
        "We'll need a few steps to preprocess our text and represent it numerically. **Why do we need to represent our text as numbers?**\n",
        "\n",
        "We'll talk through all the steps here, then use a single function to implement them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pby7LlwhEvpN"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "First of all, we would like to **tokenize** each review: convert it from a single string into a list of words. Enter some example text into the cell below to see the tokenized version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XOaa1uEEvpY",
        "outputId": "7b6624ef-0384-460b-a910-908e2111cc43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Basic tokenization example\n",
        "example_text = \"First of all, we would like to tokenize each review: convert it from a single string into a list of words. Enter some example text into the cell below to see the tokenized version.\" #@param {type:\"string\"}\n",
        "tokens = word_tokenize(example_text)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['First',\n",
              " 'of',\n",
              " 'all',\n",
              " ',',\n",
              " 'we',\n",
              " 'would',\n",
              " 'like',\n",
              " 'to',\n",
              " 'tokenize',\n",
              " 'each',\n",
              " 'review',\n",
              " ':',\n",
              " 'convert',\n",
              " 'it',\n",
              " 'from',\n",
              " 'a',\n",
              " 'single',\n",
              " 'string',\n",
              " 'into',\n",
              " 'a',\n",
              " 'list',\n",
              " 'of',\n",
              " 'words',\n",
              " '.',\n",
              " 'Enter',\n",
              " 'some',\n",
              " 'example',\n",
              " 'text',\n",
              " 'into',\n",
              " 'the',\n",
              " 'cell',\n",
              " 'below',\n",
              " 'to',\n",
              " 'see',\n",
              " 'the',\n",
              " 'tokenized',\n",
              " 'version',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKCP5Q_LEvpg"
      },
      "source": [
        "## Stopwords\n",
        "\n",
        "Next, let's remove **stopwords**: words which are there to provide grammatical structure, but don't give us much information about a review's sentiment.\n",
        "\n",
        "Edit the cell below to see if we're considering a given word as a stopword! Do you agree with the results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqq4w-ZrEvpj",
        "cellView": "form",
        "outputId": "f3aef635-0709-4341-e41a-a1533a16310e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Check if a word is a stop word\n",
        "example_word = \"1\" #@param {type:'string'}\n",
        "if example_word.lower() in STOP_WORDS:\n",
        "  print ('\"' + example_word + '\" is a stop word.')\n",
        "else:\n",
        "  print ('\"' + example_word + '\" is NOT a stop word.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"1\" is NOT a stop word.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vubl4ejEvpv"
      },
      "source": [
        "We're going to remove these stopwords from the user reviews.\n",
        "\n",
        "Tokenization and removal of stop words are universal to nearly every NLP application. In some cases, additional cleaning may be required (for example, removal of proper nouns, removal of digits) but we can build a text preprocessing function with these \"base\" cleaning steps.\n",
        "\n",
        "Putting all these together, we can come up with a text cleaning function that we can apply to all of our reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfIUrRtEWr3H"
      },
      "source": [
        "## Vectors\n",
        "\n",
        "Finally, we'll need to convert our text to **vectors**, or lists of numbers. We'll start off doing this with Bag of Words, but we'll talk about another approach later!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43hbR0vha7E3"
      },
      "source": [
        "### Bag of Words\n",
        "\n",
        "In a **bag of words** approach, we count how many times each word was used in each review.\n",
        "\n",
        "Suppose we want to represent two **reviews**: \n",
        "- \"The food was great. The ambience was also great.\"\n",
        "- \"Great ambience, but not great food!\"\n",
        "\n",
        "First we define our vocabulary. This is *each unique word* in the review. So our **vocabulary** is: \n",
        "- [also, ambience, but, food, great, not, the, was].\n",
        "\n",
        "Next, we count up how many times each word was used! (You can also think of this as adding up one-hot encodings.)\n",
        "\n",
        "Our reviews are encoded as:\n",
        "- **First review:** [1, 1, 0, 1, 2, 0, 2, 2]. Can you explain why?\n",
        "- **Second review:** [_, _, _, _, _, _, _, _] Fill it in here! \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbFU78B5bXka"
      },
      "source": [
        "## Preprocessing Our Text in Action\n",
        "\n",
        "Let's use bag-of-words to prepare our data! \n",
        "\n",
        "First, let's select our input *X* and output *y*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t6HQm1vEvrQ"
      },
      "source": [
        "X_text = yelp['text']\n",
        "y = yelp['is_good_review']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhRyg_YEeA5t"
      },
      "source": [
        "Now, let's prepare our data! First, we'll use CountVectorizer, a useful tool from Scikit-learn, to: \n",
        "*   Tokenize our reviews\n",
        "*   Remove stopwords\n",
        "*   Prepare our vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrSQAeKjAiXJ"
      },
      "source": [
        "#@title Initialize the text cleaning function { display-mode: \"form\" }\n",
        "def tokenize(text):\n",
        "    clean_tokens = []\n",
        "    for token in text_to_nlp(text):\n",
        "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct): # -PRON- is a special all inclusive \"lemma\" spaCy uses for any pronoun, we want to exclude these \n",
        "            clean_tokens.append(token.lemma_)\n",
        "    return clean_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfDtH-XTSrCK"
      },
      "source": [
        "The cell below will take a moment! **Can you guess what `max_features` is for?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blZ7RJ2zEvrU"
      },
      "source": [
        "bow_transformer = CountVectorizer(analyzer=tokenize, max_features=800).fit(X_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaheGj_RmKW7"
      },
      "source": [
        "Now, we can see our entire vocabulary! Can you guess what the numbers represent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TjdgYVxmKgd",
        "outputId": "c9af4969-0373-437b-ca0e-1f361fbb7923",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bow_transformer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " '\\n\\n': 1,\n",
              " ' ': 2,\n",
              " ' \\n': 3,\n",
              " ' \\n\\n': 4,\n",
              " '  ': 5,\n",
              " '$': 6,\n",
              " '+': 7,\n",
              " '1': 8,\n",
              " '1/2': 9,\n",
              " '10': 10,\n",
              " '12': 11,\n",
              " '15': 12,\n",
              " '2': 13,\n",
              " '20': 14,\n",
              " '25': 15,\n",
              " '3': 16,\n",
              " '30': 17,\n",
              " '4': 18,\n",
              " '5': 19,\n",
              " '50': 20,\n",
              " '6': 21,\n",
              " '7': 22,\n",
              " '8': 23,\n",
              " '9': 24,\n",
              " '=': 25,\n",
              " 'AZ': 26,\n",
              " 'Arizona': 27,\n",
              " 'BBQ': 28,\n",
              " 'Burger': 29,\n",
              " 'Chandler': 30,\n",
              " 'Chicken': 31,\n",
              " 'Chili': 32,\n",
              " 'Day': 33,\n",
              " 'Food': 34,\n",
              " 'Friday': 35,\n",
              " 'Green': 36,\n",
              " 'Grill': 37,\n",
              " 'Happy': 38,\n",
              " 'Hour': 39,\n",
              " 'Mesa': 40,\n",
              " 'New': 41,\n",
              " 'Old': 42,\n",
              " 'Phoenix': 43,\n",
              " 'Saturday': 44,\n",
              " 'Scottsdale': 45,\n",
              " 'Service': 46,\n",
              " 'Sunday': 47,\n",
              " 'Tempe': 48,\n",
              " 'Thai': 49,\n",
              " 'Town': 50,\n",
              " 'Valley': 51,\n",
              " 'Yelp': 52,\n",
              " 'able': 53,\n",
              " 'absolutely': 54,\n",
              " 'actually': 55,\n",
              " 'add': 56,\n",
              " 'admit': 57,\n",
              " 'adult': 58,\n",
              " 'afternoon': 59,\n",
              " 'ago': 60,\n",
              " 'agree': 61,\n",
              " 'amazing': 62,\n",
              " 'ambiance': 63,\n",
              " 'app': 64,\n",
              " 'appetizer': 65,\n",
              " 'area': 66,\n",
              " 'arrive': 67,\n",
              " 'art': 68,\n",
              " 'asada': 69,\n",
              " 'asian': 70,\n",
              " 'ask': 71,\n",
              " 'atmosphere': 72,\n",
              " 'attention': 73,\n",
              " 'attentive': 74,\n",
              " 'authentic': 75,\n",
              " 'available': 76,\n",
              " 'average': 77,\n",
              " 'away': 78,\n",
              " 'awesome': 79,\n",
              " 'baby': 80,\n",
              " 'bacon': 81,\n",
              " 'bad': 82,\n",
              " 'bag': 83,\n",
              " 'bagel': 84,\n",
              " 'bake': 85,\n",
              " 'ball': 86,\n",
              " 'banana': 87,\n",
              " 'bar': 88,\n",
              " 'barely': 89,\n",
              " 'bartender': 90,\n",
              " 'base': 91,\n",
              " 'basically': 92,\n",
              " 'basil': 93,\n",
              " 'bathroom': 94,\n",
              " 'bean': 95,\n",
              " 'beat': 96,\n",
              " 'beautiful': 97,\n",
              " 'beef': 98,\n",
              " 'beer': 99,\n",
              " 'believe': 100,\n",
              " 'better': 101,\n",
              " 'beverage': 102,\n",
              " 'big': 103,\n",
              " 'bill': 104,\n",
              " 'birthday': 105,\n",
              " 'bit': 106,\n",
              " 'bite': 107,\n",
              " 'black': 108,\n",
              " 'blah': 109,\n",
              " 'bland': 110,\n",
              " 'blow': 111,\n",
              " 'blue': 112,\n",
              " 'bonus': 113,\n",
              " 'book': 114,\n",
              " 'booth': 115,\n",
              " 'bottle': 116,\n",
              " 'bowl': 117,\n",
              " 'box': 118,\n",
              " 'boyfriend': 119,\n",
              " 'bread': 120,\n",
              " 'break': 121,\n",
              " 'breakfast': 122,\n",
              " 'bring': 123,\n",
              " 'brown': 124,\n",
              " 'brownie': 125,\n",
              " 'brunch': 126,\n",
              " 'buck': 127,\n",
              " 'bunch': 128,\n",
              " 'burger': 129,\n",
              " 'burn': 130,\n",
              " 'burrito': 131,\n",
              " 'business': 132,\n",
              " 'busy': 133,\n",
              " 'butter': 134,\n",
              " 'buy': 135,\n",
              " 'cafe': 136,\n",
              " 'cake': 137,\n",
              " 'call': 138,\n",
              " 'calorie': 139,\n",
              " 'car': 140,\n",
              " 'card': 141,\n",
              " 'care': 142,\n",
              " 'carne': 143,\n",
              " 'case': 144,\n",
              " 'cash': 145,\n",
              " 'casual': 146,\n",
              " 'catch': 147,\n",
              " 'chain': 148,\n",
              " 'chair': 149,\n",
              " 'chance': 150,\n",
              " 'change': 151,\n",
              " 'charge': 152,\n",
              " 'cheap': 153,\n",
              " 'check': 154,\n",
              " 'cheese': 155,\n",
              " 'chef': 156,\n",
              " 'chicken': 157,\n",
              " 'chile': 158,\n",
              " 'chili': 159,\n",
              " 'chill': 160,\n",
              " 'chinese': 161,\n",
              " 'chip': 162,\n",
              " 'chocolate': 163,\n",
              " 'choice': 164,\n",
              " 'choose': 165,\n",
              " 'city': 166,\n",
              " 'class': 167,\n",
              " 'clean': 168,\n",
              " 'clear': 169,\n",
              " 'close': 170,\n",
              " 'club': 171,\n",
              " 'coffee': 172,\n",
              " 'cold': 173,\n",
              " 'college': 174,\n",
              " 'color': 175,\n",
              " 'combo': 176,\n",
              " 'come': 177,\n",
              " 'comfortable': 178,\n",
              " 'company': 179,\n",
              " 'compare': 180,\n",
              " 'complain': 181,\n",
              " 'complaint': 182,\n",
              " 'completely': 183,\n",
              " 'consider': 184,\n",
              " 'continue': 185,\n",
              " 'conversation': 186,\n",
              " 'cook': 187,\n",
              " 'cookie': 188,\n",
              " 'cool': 189,\n",
              " 'corn': 190,\n",
              " 'corner': 191,\n",
              " 'cost': 192,\n",
              " 'counter': 193,\n",
              " 'couple': 194,\n",
              " 'coupon': 195,\n",
              " 'course': 196,\n",
              " 'cover': 197,\n",
              " 'crab': 198,\n",
              " 'crave': 199,\n",
              " 'crazy': 200,\n",
              " 'cream': 201,\n",
              " 'creamy': 202,\n",
              " 'create': 203,\n",
              " 'crispy': 204,\n",
              " 'crowd': 205,\n",
              " 'crust': 206,\n",
              " 'cup': 207,\n",
              " 'cupcake': 208,\n",
              " 'curry': 209,\n",
              " 'customer': 210,\n",
              " 'cut': 211,\n",
              " 'cute': 212,\n",
              " 'damn': 213,\n",
              " 'dance': 214,\n",
              " 'dark': 215,\n",
              " 'date': 216,\n",
              " 'daughter': 217,\n",
              " 'day': 218,\n",
              " 'de': 219,\n",
              " 'deal': 220,\n",
              " 'decent': 221,\n",
              " 'decide': 222,\n",
              " 'decor': 223,\n",
              " 'deep': 224,\n",
              " 'definitely': 225,\n",
              " 'delicious': 226,\n",
              " 'delivery': 227,\n",
              " 'desert': 228,\n",
              " 'dessert': 229,\n",
              " 'die': 230,\n",
              " 'difference': 231,\n",
              " 'different': 232,\n",
              " 'difficult': 233,\n",
              " 'dining': 234,\n",
              " 'dinner': 235,\n",
              " 'dip': 236,\n",
              " 'dirty': 237,\n",
              " 'disappoint': 238,\n",
              " 'disappointed': 239,\n",
              " 'dish': 240,\n",
              " 'dog': 241,\n",
              " 'dollar': 242,\n",
              " 'door': 243,\n",
              " 'downtown': 244,\n",
              " 'dressing': 245,\n",
              " 'drink': 246,\n",
              " 'drive': 247,\n",
              " 'drop': 248,\n",
              " 'dry': 249,\n",
              " 'duck': 250,\n",
              " 'early': 251,\n",
              " 'easily': 252,\n",
              " 'easy': 253,\n",
              " 'eat': 254,\n",
              " 'egg': 255,\n",
              " 'employee': 256,\n",
              " 'end': 257,\n",
              " 'enjoy': 258,\n",
              " 'enter': 259,\n",
              " 'entire': 260,\n",
              " 'entree': 261,\n",
              " 'especially': 262,\n",
              " 'establishment': 263,\n",
              " 'etc': 264,\n",
              " 'evening': 265,\n",
              " 'event': 266,\n",
              " 'exactly': 267,\n",
              " 'excellent': 268,\n",
              " 'excited': 269,\n",
              " 'expect': 270,\n",
              " 'expectation': 271,\n",
              " 'expensive': 272,\n",
              " 'experience': 273,\n",
              " 'explain': 274,\n",
              " 'extensive': 275,\n",
              " 'extra': 276,\n",
              " 'extremely': 277,\n",
              " 'eye': 278,\n",
              " 'fabulous': 279,\n",
              " 'face': 280,\n",
              " 'fact': 281,\n",
              " 'fairly': 282,\n",
              " 'fall': 283,\n",
              " 'family': 284,\n",
              " 'fan': 285,\n",
              " 'fancy': 286,\n",
              " 'fantastic': 287,\n",
              " 'far': 288,\n",
              " 'fast': 289,\n",
              " 'fat': 290,\n",
              " 'favorite': 291,\n",
              " 'feel': 292,\n",
              " 'feeling': 293,\n",
              " 'figure': 294,\n",
              " 'fill': 295,\n",
              " 'finally': 296,\n",
              " 'find': 297,\n",
              " 'fine': 298,\n",
              " 'finish': 299,\n",
              " 'fish': 300,\n",
              " 'fit': 301,\n",
              " 'fix': 302,\n",
              " 'flavor': 303,\n",
              " 'flavorful': 304,\n",
              " 'floor': 305,\n",
              " 'folk': 306,\n",
              " 'follow': 307,\n",
              " 'food': 308,\n",
              " 'forget': 309,\n",
              " 'forward': 310,\n",
              " 'fountain': 311,\n",
              " 'free': 312,\n",
              " 'french': 313,\n",
              " 'fresh': 314,\n",
              " 'freshly': 315,\n",
              " 'friend': 316,\n",
              " 'friendly': 317,\n",
              " 'fruit': 318,\n",
              " 'fry': 319,\n",
              " 'fun': 320,\n",
              " 'game': 321,\n",
              " 'garlic': 322,\n",
              " 'gel': 323,\n",
              " 'generally': 324,\n",
              " 'get': 325,\n",
              " 'gift': 326,\n",
              " 'girl': 327,\n",
              " 'girlfriend': 328,\n",
              " 'give': 329,\n",
              " 'glad': 330,\n",
              " 'glass': 331,\n",
              " 'go': 332,\n",
              " 'good': 333,\n",
              " 'goodness': 334,\n",
              " 'grab': 335,\n",
              " 'gravy': 336,\n",
              " 'greasy': 337,\n",
              " 'great': 338,\n",
              " 'green': 339,\n",
              " 'grill': 340,\n",
              " 'gross': 341,\n",
              " 'group': 342,\n",
              " 'grow': 343,\n",
              " 'guacamole': 344,\n",
              " 'guess': 345,\n",
              " 'guy': 346,\n",
              " 'half': 347,\n",
              " 'hand': 348,\n",
              " 'hang': 349,\n",
              " 'happen': 350,\n",
              " 'happy': 351,\n",
              " 'hard': 352,\n",
              " 'hate': 353,\n",
              " 'have': 354,\n",
              " 'head': 355,\n",
              " 'healthy': 356,\n",
              " 'hear': 357,\n",
              " 'help': 358,\n",
              " 'helpful': 359,\n",
              " 'high': 360,\n",
              " 'highly': 361,\n",
              " 'hit': 362,\n",
              " 'hold': 363,\n",
              " 'home': 364,\n",
              " 'homemade': 365,\n",
              " 'honestly': 366,\n",
              " 'hope': 367,\n",
              " 'hot': 368,\n",
              " 'hotel': 369,\n",
              " 'hour': 370,\n",
              " 'house': 371,\n",
              " 'huge': 372,\n",
              " 'hummus': 373,\n",
              " 'hungry': 374,\n",
              " 'husband': 375,\n",
              " 'ice': 376,\n",
              " 'idea': 377,\n",
              " 'immediately': 378,\n",
              " 'impressed': 379,\n",
              " 'include': 380,\n",
              " 'incredible': 381,\n",
              " 'incredibly': 382,\n",
              " 'indian': 383,\n",
              " 'ingredient': 384,\n",
              " 'inside': 385,\n",
              " 'instead': 386,\n",
              " 'interesting': 387,\n",
              " 'interior': 388,\n",
              " 'issue': 389,\n",
              " 'item': 390,\n",
              " 'job': 391,\n",
              " 'joint': 392,\n",
              " 'keep': 393,\n",
              " 'kick': 394,\n",
              " 'kid': 395,\n",
              " 'kind': 396,\n",
              " 'kinda': 397,\n",
              " 'kitchen': 398,\n",
              " 'know': 399,\n",
              " 'knowledgeable': 400,\n",
              " 'lack': 401,\n",
              " 'lady': 402,\n",
              " 'large': 403,\n",
              " 'late': 404,\n",
              " 'later': 405,\n",
              " 'leave': 406,\n",
              " 'let': 407,\n",
              " 'lettuce': 408,\n",
              " 'level': 409,\n",
              " 'life': 410,\n",
              " 'light': 411,\n",
              " 'like': 412,\n",
              " 'limited': 413,\n",
              " 'line': 414,\n",
              " 'list': 415,\n",
              " 'literally': 416,\n",
              " 'little': 417,\n",
              " 'live': 418,\n",
              " 'load': 419,\n",
              " 'local': 420,\n",
              " 'locate': 421,\n",
              " 'location': 422,\n",
              " 'long': 423,\n",
              " 'longer': 424,\n",
              " 'look': 425,\n",
              " 'lose': 426,\n",
              " 'lot': 427,\n",
              " 'loud': 428,\n",
              " 'lounge': 429,\n",
              " 'love': 430,\n",
              " 'low': 431,\n",
              " 'lucky': 432,\n",
              " 'lunch': 433,\n",
              " 'machine': 434,\n",
              " 'main': 435,\n",
              " 'major': 436,\n",
              " 'make': 437,\n",
              " 'mall': 438,\n",
              " 'man': 439,\n",
              " 'management': 440,\n",
              " 'manager': 441,\n",
              " 'manicure': 442,\n",
              " 'margarita': 443,\n",
              " 'market': 444,\n",
              " 'massage': 445,\n",
              " 'matter': 446,\n",
              " 'maybe': 447,\n",
              " 'meal': 448,\n",
              " 'mean': 449,\n",
              " 'meat': 450,\n",
              " 'meatball': 451,\n",
              " 'meet': 452,\n",
              " 'melt': 453,\n",
              " 'mention': 454,\n",
              " 'menu': 455,\n",
              " 'mexican': 456,\n",
              " 'middle': 457,\n",
              " 'milk': 458,\n",
              " 'mind': 459,\n",
              " 'minute': 460,\n",
              " 'miss': 461,\n",
              " 'mistake': 462,\n",
              " 'mix': 463,\n",
              " 'modern': 464,\n",
              " 'mom': 465,\n",
              " 'money': 466,\n",
              " 'month': 467,\n",
              " 'mood': 468,\n",
              " 'morning': 469,\n",
              " 'mouth': 470,\n",
              " 'move': 471,\n",
              " 'movie': 472,\n",
              " 'mushroom': 473,\n",
              " 'music': 474,\n",
              " 'nail': 475,\n",
              " 'near': 476,\n",
              " 'need': 477,\n",
              " 'neighborhood': 478,\n",
              " 'new': 479,\n",
              " 'nice': 480,\n",
              " 'nicely': 481,\n",
              " 'night': 482,\n",
              " 'non': 483,\n",
              " 'noodle': 484,\n",
              " 'normal': 485,\n",
              " 'normally': 486,\n",
              " 'not': 487,\n",
              " 'note': 488,\n",
              " 'notice': 489,\n",
              " 'number': 490,\n",
              " 'nut': 491,\n",
              " 'occasion': 492,\n",
              " 'offer': 493,\n",
              " 'office': 494,\n",
              " 'oh': 495,\n",
              " 'oil': 496,\n",
              " 'ok': 497,\n",
              " 'okay': 498,\n",
              " 'old': 499,\n",
              " 'olive': 500,\n",
              " 'one': 501,\n",
              " 'onion': 502,\n",
              " 'open': 503,\n",
              " 'option': 504,\n",
              " 'order': 505,\n",
              " 'original': 506,\n",
              " 'outdoor': 507,\n",
              " 'outside': 508,\n",
              " 'outstanding': 509,\n",
              " 'overall': 510,\n",
              " 'owner': 511,\n",
              " 'p.m.': 512,\n",
              " 'pack': 513,\n",
              " 'pair': 514,\n",
              " 'paper': 515,\n",
              " 'park': 516,\n",
              " 'parking': 517,\n",
              " 'party': 518,\n",
              " 'pass': 519,\n",
              " 'past': 520,\n",
              " 'pasta': 521,\n",
              " 'patio': 522,\n",
              " 'pay': 523,\n",
              " 'peanut': 524,\n",
              " 'people': 525,\n",
              " 'pepper': 526,\n",
              " 'perfect': 527,\n",
              " 'perfectly': 528,\n",
              " 'person': 529,\n",
              " 'personal': 530,\n",
              " 'pho': 531,\n",
              " 'phone': 532,\n",
              " 'pick': 533,\n",
              " 'pickle': 534,\n",
              " 'pie': 535,\n",
              " 'piece': 536,\n",
              " 'pizza': 537,\n",
              " 'place': 538,\n",
              " 'plan': 539,\n",
              " 'plate': 540,\n",
              " 'play': 541,\n",
              " 'pleasant': 542,\n",
              " 'pleased': 543,\n",
              " 'plenty': 544,\n",
              " 'plus': 545,\n",
              " 'point': 546,\n",
              " 'pool': 547,\n",
              " 'poor': 548,\n",
              " 'pop': 549,\n",
              " 'pork': 550,\n",
              " 'portion': 551,\n",
              " 'positive': 552,\n",
              " 'possible': 553,\n",
              " 'potato': 554,\n",
              " 'prefer': 555,\n",
              " 'prepare': 556,\n",
              " 'pretty': 557,\n",
              " 'price': 558,\n",
              " 'pricey': 559,\n",
              " 'probably': 560,\n",
              " 'problem': 561,\n",
              " 'product': 562,\n",
              " 'professional': 563,\n",
              " 'provide': 564,\n",
              " 'pull': 565,\n",
              " 'purchase': 566,\n",
              " 'quality': 567,\n",
              " 'question': 568,\n",
              " 'quick': 569,\n",
              " 'quickly': 570,\n",
              " 'range': 571,\n",
              " 'rate': 572,\n",
              " 'rave': 573,\n",
              " 'read': 574,\n",
              " 'ready': 575,\n",
              " 'real': 576,\n",
              " 'realize': 577,\n",
              " 'reason': 578,\n",
              " 'reasonable': 579,\n",
              " 'receive': 580,\n",
              " 'recommend': 581,\n",
              " 'recommendation': 582,\n",
              " 'red': 583,\n",
              " 'refill': 584,\n",
              " 'register': 585,\n",
              " 'regular': 586,\n",
              " 'remember': 587,\n",
              " 'remind': 588,\n",
              " 'request': 589,\n",
              " 'reservation': 590,\n",
              " 'rest': 591,\n",
              " 'restaurant': 592,\n",
              " 'return': 593,\n",
              " 'review': 594,\n",
              " 'rib': 595,\n",
              " 'rice': 596,\n",
              " 'ride': 597,\n",
              " 'right': 598,\n",
              " 'rock': 599,\n",
              " 'roll': 600,\n",
              " 'room': 601,\n",
              " 'round': 602,\n",
              " 'run': 603,\n",
              " 'salad': 604,\n",
              " 'salmon': 605,\n",
              " 'salsa': 606,\n",
              " 'salt': 607,\n",
              " 'sample': 608,\n",
              " 'sandwich': 609,\n",
              " 'sauce': 610,\n",
              " 'sausage': 611,\n",
              " 'say': 612,\n",
              " 'school': 613,\n",
              " 'screen': 614,\n",
              " 'seafood': 615,\n",
              " 'season': 616,\n",
              " 'seat': 617,\n",
              " 'seating': 618,\n",
              " 'second': 619,\n",
              " 'section': 620,\n",
              " 'see': 621,\n",
              " 'selection': 622,\n",
              " 'sell': 623,\n",
              " 'send': 624,\n",
              " 'seriously': 625,\n",
              " 'serve': 626,\n",
              " 'server': 627,\n",
              " 'service': 628,\n",
              " 'set': 629,\n",
              " 'share': 630,\n",
              " 'shirt': 631,\n",
              " 'shop': 632,\n",
              " 'shopping': 633,\n",
              " 'short': 634,\n",
              " 'shot': 635,\n",
              " 'show': 636,\n",
              " 'shrimp': 637,\n",
              " 'side': 638,\n",
              " 'sign': 639,\n",
              " 'simple': 640,\n",
              " 'simply': 641,\n",
              " 'single': 642,\n",
              " 'sit': 643,\n",
              " 'size': 644,\n",
              " 'sized': 645,\n",
              " 'slice': 646,\n",
              " 'slow': 647,\n",
              " 'small': 648,\n",
              " 'smell': 649,\n",
              " 'smile': 650,\n",
              " 'smoke': 651,\n",
              " 'soft': 652,\n",
              " 'soon': 653,\n",
              " 'sorry': 654,\n",
              " 'sort': 655,\n",
              " 'sound': 656,\n",
              " 'soup': 657,\n",
              " 'sour': 658,\n",
              " 'space': 659,\n",
              " 'speak': 660,\n",
              " 'special': 661,\n",
              " 'spend': 662,\n",
              " 'spice': 663,\n",
              " 'spicy': 664,\n",
              " 'spinach': 665,\n",
              " 'split': 666,\n",
              " 'spot': 667,\n",
              " 'spring': 668,\n",
              " 'staff': 669,\n",
              " 'stand': 670,\n",
              " 'standard': 671,\n",
              " 'star': 672,\n",
              " 'start': 673,\n",
              " 'state': 674,\n",
              " 'stay': 675,\n",
              " 'steak': 676,\n",
              " 'step': 677,\n",
              " 'stick': 678,\n",
              " 'stop': 679,\n",
              " 'store': 680,\n",
              " 'street': 681,\n",
              " 'strip': 682,\n",
              " 'strong': 683,\n",
              " 'stuff': 684,\n",
              " 'style': 685,\n",
              " 'sub': 686,\n",
              " 'suck': 687,\n",
              " 'suggest': 688,\n",
              " 'summer': 689,\n",
              " 'super': 690,\n",
              " 'suppose': 691,\n",
              " 'sure': 692,\n",
              " 'surprised': 693,\n",
              " 'sushi': 694,\n",
              " 'sweet': 695,\n",
              " 'table': 696,\n",
              " 'taco': 697,\n",
              " 'take': 698,\n",
              " 'talk': 699,\n",
              " 'taste': 700,\n",
              " 'tasting': 701,\n",
              " 'tasty': 702,\n",
              " 'tea': 703,\n",
              " 'tell': 704,\n",
              " 'tender': 705,\n",
              " 'texture': 706,\n",
              " 'thank': 707,\n",
              " 'thick': 708,\n",
              " 'thin': 709,\n",
              " 'thing': 710,\n",
              " 'think': 711,\n",
              " 'throw': 712,\n",
              " 'time': 713,\n",
              " 'tiny': 714,\n",
              " 'tip': 715,\n",
              " 'to': 716,\n",
              " 'toast': 717,\n",
              " 'today': 718,\n",
              " 'tomato': 719,\n",
              " 'ton': 720,\n",
              " 'top': 721,\n",
              " 'topping': 722,\n",
              " 'tortilla': 723,\n",
              " 'total': 724,\n",
              " 'totally': 725,\n",
              " 'touch': 726,\n",
              " 'town': 727,\n",
              " 'trail': 728,\n",
              " 'travel': 729,\n",
              " 'treat': 730,\n",
              " 'trendy': 731,\n",
              " 'trip': 732,\n",
              " 'truly': 733,\n",
              " 'truth': 734,\n",
              " 'try': 735,\n",
              " 'turn': 736,\n",
              " 'tv': 737,\n",
              " 'twice': 738,\n",
              " 'type': 739,\n",
              " 'typical': 740,\n",
              " 'understand': 741,\n",
              " 'unfortunately': 742,\n",
              " 'unique': 743,\n",
              " 'use': 744,\n",
              " 'usual': 745,\n",
              " 'usually': 746,\n",
              " 'valley': 747,\n",
              " 'value': 748,\n",
              " 'variety': 749,\n",
              " 'vegan': 750,\n",
              " 'vegetable': 751,\n",
              " 'vegetarian': 752,\n",
              " 'veggie': 753,\n",
              " 'venue': 754,\n",
              " 'vibe': 755,\n",
              " 'view': 756,\n",
              " 'visit': 757,\n",
              " 'waffle': 758,\n",
              " 'wait': 759,\n",
              " 'waiter': 760,\n",
              " 'waitress': 761,\n",
              " 'walk': 762,\n",
              " 'wall': 763,\n",
              " 'want': 764,\n",
              " 'warm': 765,\n",
              " 'watch': 766,\n",
              " 'water': 767,\n",
              " 'way': 768,\n",
              " 'wear': 769,\n",
              " 'website': 770,\n",
              " 'wedding': 771,\n",
              " 'week': 772,\n",
              " 'weekend': 773,\n",
              " 'weird': 774,\n",
              " 'welcome': 775,\n",
              " 'well': 776,\n",
              " 'white': 777,\n",
              " 'wide': 778,\n",
              " 'wife': 779,\n",
              " 'will': 780,\n",
              " 'window': 781,\n",
              " 'wine': 782,\n",
              " 'wing': 783,\n",
              " 'wish': 784,\n",
              " 'woman': 785,\n",
              " 'wonderful': 786,\n",
              " 'word': 787,\n",
              " 'work': 788,\n",
              " 'world': 789,\n",
              " 'worth': 790,\n",
              " 'wow': 791,\n",
              " 'write': 792,\n",
              " 'wrong': 793,\n",
              " 'yeah': 794,\n",
              " 'year': 795,\n",
              " 'yelp': 796,\n",
              " 'yes': 797,\n",
              " 'yum': 798,\n",
              " 'yummy': 799}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGEf8h1lEvrY"
      },
      "source": [
        "The number represents the **index** (alphabetical position) of a word in the vocabulary.\n",
        "\n",
        "By the way, how many words do we have?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upN1gxm5Evrb",
        "outputId": "21fd6aa9-dc29-44fc-ef94-44c23c4b59b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(bow_transformer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkN4E-QCTFom"
      },
      "source": [
        "It's the same as `max_features`! Is that a coincidence? What's the point of `max_features`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsFa7Nu5Evr4"
      },
      "source": [
        "Now that our vocabulary is ready, we can **transform** each review into a bag of words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRJJe2HGEvr6"
      },
      "source": [
        "X = bow_transformer.transform(X_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu6FEly_UMFC"
      },
      "source": [
        "Finally, we've converted our reviews to numerical data that we can use in a logistic regression!\n",
        "\n",
        "We can see what `X` looks like by printing it out as a DataFrame. **How long is each review's vector?** Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdeKo8ZQTiOu",
        "outputId": "744ec609-98e7-4ada-fa2f-969b3064f519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "pd.DataFrame(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f18b75d2-efde-4892-b696-d66913715b57\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "      <th>785</th>\n",
              "      <th>786</th>\n",
              "      <th>787</th>\n",
              "      <th>788</th>\n",
              "      <th>789</th>\n",
              "      <th>790</th>\n",
              "      <th>791</th>\n",
              "      <th>792</th>\n",
              "      <th>793</th>\n",
              "      <th>794</th>\n",
              "      <th>795</th>\n",
              "      <th>796</th>\n",
              "      <th>797</th>\n",
              "      <th>798</th>\n",
              "      <th>799</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 800 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f18b75d2-efde-4892-b696-d66913715b57')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f18b75d2-efde-4892-b696-d66913715b57 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f18b75d2-efde-4892-b696-d66913715b57');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     0    1    2    3    4    5    6    ...  793  794  795  796  797  798  799\n",
              "0      0    3    8    0    0    0    0  ...    0    0    0    0    0    0    0\n",
              "1      0    2    1    0    0    0    0  ...    0    0    0    0    0    0    0\n",
              "2      0    2    1    0    0    0    0  ...    0    0    0    0    0    0    0\n",
              "3      0    1    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
              "4      0    3   18    0    0    0    0  ...    0    0    0    0    0    0    1\n",
              "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
              "995    0    0    2    0    1    0    0  ...    0    0    1    0    0    0    0\n",
              "996    0    0    1    0    0    0    0  ...    0    0    0    0    0    0    0\n",
              "997    0   11    0    0    0    0    0  ...    0    1    0    0    0    0    0\n",
              "998    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
              "999    3    0    3    0    0    0    3  ...    0    0    0    0    0    0    0\n",
              "\n",
              "[1000 rows x 800 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mLW7krRcBIG"
      },
      "source": [
        "## Interlude: Word Embeddings\n",
        "\n",
        "By the way, there's another way we could have converted our text to vectors: using **word embeddings** like Word2Vec. \n",
        "\n",
        "Let's explore word embeddings using a library called Spacy, which comes built-in with lots of useful information about the English language. Let's prepare a useful function from Spacy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nuu0QFg-VOiN"
      },
      "source": [
        "text_to_nlp = en_core_web_md.load() #Prepare Spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYov_nu-VXQn"
      },
      "source": [
        "`text_to_nlp` lets us find lots of information about a sentence. For example, we can pick out a specific word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldEPkz8NcI6_",
        "outputId": "8d784028-2bae-49ec-d037-abffaf4f5103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "doc = text_to_nlp(\"I like apples and cherries and peaches and pie\")\n",
        "token = doc[2] #Try changing this!\n",
        "print (token)\n",
        "print (len(token))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apples\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWyXczXkePiI"
      },
      "source": [
        "We can also find the **word embedding** for each word: a 300-dimensional vector that captures the word's meaning!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0tBmojQeNhn",
        "outputId": "e3cfcf9a-91fa-4d49-ea6e-0a3d1fa6d072",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print ('Vector for: ', token)\n",
        "print (token.vector) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for:  apples\n",
            "[-0.6334     0.18981   -0.53544   -0.52658   -0.30001    0.30559\n",
            " -0.49303    0.14636    0.012273   0.96802    0.0040354  0.25234\n",
            " -0.29864   -0.014646  -0.24905   -0.67125   -0.053366   0.59426\n",
            " -0.068034   0.10315    0.66759    0.024617  -0.37548    0.52557\n",
            "  0.054449  -0.36748   -0.28013    0.090898  -0.025687  -0.5947\n",
            " -0.24269    0.28603    0.686      0.29737    0.30422    0.69032\n",
            "  0.042784   0.023701  -0.57165    0.70581   -0.20813   -0.03204\n",
            " -0.12494   -0.42933    0.31271    0.30352    0.09421   -0.15493\n",
            "  0.071356   0.15022   -0.41792    0.066394  -0.034546  -0.45772\n",
            "  0.57177   -0.82755   -0.27885    0.71801   -0.12425    0.18551\n",
            "  0.41342   -0.53997    0.55864   -0.015805  -0.1074    -0.29981\n",
            " -0.17271    0.27066    0.043996   0.60107   -0.353      0.6831\n",
            "  0.20703    0.12068    0.24852   -0.15605    0.25812    0.007004\n",
            " -0.10741   -0.097053   0.085628   0.096307   0.20857   -0.23338\n",
            " -0.077905  -0.030906   1.0494     0.55368   -0.10703    0.052234\n",
            "  0.43407   -0.13926    0.38115    0.021104  -0.40922    0.35972\n",
            " -0.28898    0.30618    0.060807  -0.023517   0.58193   -0.3098\n",
            "  0.21013   -0.15557   -0.56913   -1.1364     0.36598   -0.032666\n",
            "  1.1926     0.12825   -0.090486  -0.47965   -0.61164   -0.16484\n",
            " -0.41134    0.19925    0.059183  -0.20842    0.45223    0.27697\n",
            " -0.20745    0.025404  -0.28874    0.040478  -0.22275   -0.43323\n",
            "  0.76957   -0.054327  -0.35213   -0.30842   -0.48791   -0.35564\n",
            "  0.19813   -0.094767  -0.50918    0.18763   -0.087555   0.37709\n",
            " -0.1322    -0.096913  -1.9102     0.55813    0.27391   -0.077744\n",
            " -0.43933   -0.10367   -0.24408    0.41869    0.11659    0.27454\n",
            "  0.81021   -0.11006    0.43131    0.29095   -0.49548   -0.31958\n",
            " -0.072506   0.020286   0.2179     0.22032   -0.29212    0.75639\n",
            "  0.13598    0.019736  -0.83104    0.22836   -0.28669   -1.0529\n",
            "  0.052771   0.41266    0.50149    0.5323     0.51573   -0.31806\n",
            " -0.4619     0.21739   -0.43584   -0.41382    0.042237  -0.57179\n",
            "  0.067623  -0.27854    0.090044   0.20633    0.024678  -0.57703\n",
            " -0.020183  -0.53147   -0.37548   -0.12795   -0.093662  -0.0061183\n",
            "  0.20221   -0.62296   -0.29746    0.26935    0.59009   -0.50382\n",
            " -0.69757    0.20157   -0.33592   -0.45766    0.14061    0.22982\n",
            "  0.044046   0.26386    0.02942    0.34095    1.1496    -0.15555\n",
            " -0.064071   0.30139    0.024211  -0.63515   -0.73347   -0.10346\n",
            " -0.22637   -0.056392  -0.16735   -0.097331  -0.19206   -0.18866\n",
            "  0.15116   -0.038048   0.70205    0.11586   -0.14813    0.0095166\n",
            " -0.33804   -0.10158   -0.23829   -0.22759    0.092504  -0.29839\n",
            " -0.39721    0.26092    0.34594   -0.47396   -0.25725   -0.19257\n",
            " -0.53071    0.1692    -0.47252   -0.17333   -0.40505    0.046446\n",
            " -0.04473    0.33555   -0.5693     0.31591   -0.21167   -0.31298\n",
            " -0.45923   -0.083091   0.086822   0.01264    0.43779    0.12651\n",
            "  0.30156    0.022061   0.26549   -0.29455   -0.14838    0.033692\n",
            " -0.37346   -0.075343  -0.56498   -0.24207   -0.69351   -0.20277\n",
            " -0.0081185  0.030971   0.53615   -0.16613   -0.84087    0.74661\n",
            "  0.029132   0.46936   -0.49755    0.40954   -0.022558   0.21497\n",
            " -0.049528  -0.039799   0.46165    0.26456    0.32985   -0.04219\n",
            " -0.099599  -0.17312   -0.476     -0.019048  -0.41888   -0.2685\n",
            " -0.65281    0.068773  -0.23881   -1.1784     0.25504    0.61171  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBagECaVMQXZ"
      },
      "source": [
        "#### Exercise: Exploring Similarity\n",
        "\n",
        "A neat thing you can do with word embeddings is calculate similarity, like this:\n",
        "\n",
        "```\n",
        "doc = text_to_nlp(u\"keyboard and mouse\")\n",
        "word1 = doc[0]\n",
        "word2 = doc[2]\n",
        "word1.similarity(word2)\n",
        "```\n",
        "\n",
        "Using the example above, try to find **two words with similarity greater than .80** and **two words with similarity less than .15**!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD8fVPwlM4Bn",
        "outputId": "4b7fa704-7071-4f1c-b395-5bea94f54c5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### YOUR CODE HERE\n",
        "doc = text_to_nlp(u\"universe and cosmos\")\n",
        "word1 = doc[0]\n",
        "word2 = doc[2]\n",
        "word1.similarity(word2)\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1myx2wBXQmH"
      },
      "source": [
        "Check out the challenge exercises if you're interested in exploring word embeddings further with our Yelp dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8AUxyLHEvr_"
      },
      "source": [
        "# Creating a Baseline Classifier\n",
        "\n",
        "Now, back to our sentiment analysis problem! Our data is ready for machine learning.\n",
        "\n",
        "Our classification problem is a classic two-class classification problem, and so we will use the tried-and-tested **Logistic Regression** machine learning model.\n",
        "\n",
        "As always, we'll start by setting aside testing and training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PThy6pNUEvsA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "62a35591-4b5e-4235-a619-e5f0660bce5f"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-503f7d0a978c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42rl41sUXr0-"
      },
      "source": [
        "###Exercise: Training your Model\n",
        "Now, we can create and train our model! Please **fill in the code to train (or *fit*) your model**.\n",
        "\n",
        "(Need a hint? Refer to last time's notebook or Scikit-learn documentation if needed.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIpbdNZwgRTn",
        "outputId": "c021ecee-ae8e-4f70-c666-38f3ff12f291",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "logistic_model = LogisticRegression()\n",
        "\n",
        "#YOUR CODE HERE to train the model (1 line)\n",
        "logistic_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hFMR7NUV84D"
      },
      "source": [
        "###Exercise: Testing Your Model\n",
        "Now, let's evaluate our model's accuracy! Your model needs to **predict** the sentiment, and then you'll **calculate the accuracy** using `accuracy_score`. **Which dataset** should you use?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVdf84tuWQwM",
        "outputId": "f680db8c-d894-4026-e24a-efe7851e8f16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred = logistic_model.predict(X_test) #YOUR CODE HERE\n",
        "accuracy = accuracy_score(y_test, y_pred) #YOUR CODE HERE\n",
        "print (accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zct-JZPQaXno"
      },
      "source": [
        "Congratulations - you've trained and tested your model! It's not perfect, but a whole lot better than a coin flip :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILybPDLAaphw"
      },
      "source": [
        "#Optional: Exploring Your Model\n",
        "\n",
        "Let's explore your model in more depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiUFnTXrased"
      },
      "source": [
        "###Exercise: Trying Out Reviews\n",
        "\n",
        "Accuracy only tells us so much! It's often useful to figure out **what sorts** of mistakes your model makes. \n",
        "\n",
        "Try entering some reviews below and explore:\n",
        "\n",
        "*   What kind of reviews does your model classify correctly? For example, do long or short reviews work better?\n",
        "*   What kind of reviews does your model get wrong? Does it understand sarcasm or other \"tricky\" language?\n",
        "*   Does it seem like your model pays attention to particular words?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euuR1VWWEvsX"
      },
      "source": [
        "#@title Enter a review to see your model's classification\n",
        "example_review = \"\" #@param {type:'string'}\n",
        "prediction = logistic_model.predict(bow_transformer.transform([example_review]))\n",
        "\n",
        "if prediction:\n",
        "  print (\"This was a GOOD review!\")\n",
        "else:\n",
        "  print (\"This was a BAD review!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV0Fe3GpHd2_"
      },
      "source": [
        "###Exercise: Changing the Vocabulary Size\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VY4ovilmDJaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1i_LdIeHqgo"
      },
      "source": [
        "Experiment with changing the `max_features` attribute when you created the bag of words model: the maximum size of the vocabulary. Then re-train your model (you might find \"Runtime > Run after\" useful.) \n",
        "\n",
        "Discuss: how does this change affect the accuracy of your model? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k-2NwiLHL4w"
      },
      "source": [
        "###Exercise: Using a Different Classifier\n",
        "\n",
        "We used logistic regression for our baseline model, but there are many other classifier models we could use!\n",
        "\n",
        "One common model is called Multinomial Naive Bayes. Naive Bayes uses Bayes' Theorem of probability to predict the class of new input data. The important assumption that Naive Bayes makes is that all the features are independent: the number of times a review uses \"potato\" is unrelated to the number of times a review uses \"server\". (Do you think this is an accurate assumption??)\n",
        "\n",
        "Let's build a model using a Naive Bayes classifier!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPg0Y7cjHH2c"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb_model = MultinomialNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nqtNo_qIsbh"
      },
      "source": [
        "We can train and generate predictions from this model in the same way we did for our Logistic Regression model. Try training this model on the same data and see if it performs better or worse than our logistic regression model. Then, evaluate the model accuracy as you did for the Logistic Regression classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q-UOCh0I0BR"
      },
      "source": [
        "###YOUR CODE HERE####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TffS4NT-BSS"
      },
      "source": [
        "Experiment with [other models](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) to try to get the highest accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2-jsJ5F-B-a"
      },
      "source": [
        "###YOUR CODE HERE###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFeI61HGY2Kt"
      },
      "source": [
        "# Optional (Advanced): Training Logistic Regression with Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9DUXQXSaMPU"
      },
      "source": [
        "As we've discussed, there's an alternative to bag-of-words: we can use word embeddings to get more sophisticated representations of our reviews. Let's try it out!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_n7uJ20a_yH"
      },
      "source": [
        "We'll use this helper function to remove stop words, pronouns, and punctuation, and convert each word to a spaCy object (we can use `token.vector` later)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAOs86fEQ_2l"
      },
      "source": [
        "def tokenize_vecs(text):\n",
        "    clean_tokens = []\n",
        "    for token in text_to_nlp(text):\n",
        "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct): \n",
        "          # -PRON- is a special all inclusive \"lemma\" spaCy uses for any pronoun, we want to exclude these \n",
        "            clean_tokens.append(token)\n",
        "    return clean_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0XqDYs_b4V8"
      },
      "source": [
        "We want to represent each Yelp review with a vector. Since each review consists of multiple words, we want to find a way to create one vector for each review. \n",
        "\n",
        "Would adding the word vectors work? What about averaging? Which would be preferrable?\n",
        "\n",
        "Implement your solution below: convert our array of reviews into an array of vector representations of those reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tKKKLymMNsd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "1409c380-f229-4e0d-ae6e-d258946e3d07"
      },
      "source": [
        "X_word2vec = []\n",
        "for text in X_text:\n",
        "  review = tokenize_vecs(text) # returns cleaned list of spacy tokens\n",
        "  #### YOUR CODE HERE\n",
        "  \n",
        "  \n",
        "  #### END CODE\n",
        "  \n",
        "X_word2vec = np.array(X_word2vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3733ab9cfa5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_word2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_vecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# returns cleaned list of spacy tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#### YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frxz5RO5c0ft"
      },
      "source": [
        "Now, test and train a logistic regression mode! Remember to create new training and testing data using `X_word2vec`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c7m639IGVuT"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehdlMAFaHByC"
      },
      "source": [
        "**Discuss:** Can you explain the results? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcqbg3NSNriB"
      },
      "source": [
        "# Optional (Challenge): Word Embedding Math\n",
        "\n",
        "(Heads-up: this challenge section is math-heavy!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTyeFBh0CbNO"
      },
      "source": [
        "One reason text embeddings are cool is that we can use them to explore connections in meaning between different words, including calculating similarity between words and completing [analogies](http://bionlp-www.utu.fi/wv_demo/).\n",
        "\n",
        "We'll start by creating a dictionary containing the vectors for all the words in our vocabulary. We'll stick to the vocabulary above of 800 words from the Yelp reviews - if you want to use more words, change that number! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHlGBkBKoUPI"
      },
      "source": [
        "vocab_dict = dict() #initialize dictionary\n",
        "\n",
        "for word in bow_transformer.vocabulary_:\n",
        "    vocab_dict[word] = text_to_nlp(word).vector # What is the key? What is the value?\n",
        "\n",
        "for word, vec in vocab_dict.items(): # Iterating through the dictionary to print each key and value\n",
        "  print ('Word: {}. Vector length: {}'.format(word, len(vec)))\n",
        "\n",
        "print()\n",
        "print ('{} words in our dictionary'.format(len(vocab_dict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPCcIyTpDuab"
      },
      "source": [
        "Next, let's calculate the similarity between two words, using their Word2Vec representations.\n",
        "\n",
        "A common way to calculate the similarity between two vectors is called *cosine similarity*. It depends on the angle between those two vectors when plotted in space. As an example, imagine we had two three-dimensional vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omAmAv88GZUp"
      },
      "source": [
        "v0 = [2,3,1]\n",
        "v1 = [2,4,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owbBQZUgGgjs"
      },
      "source": [
        "Run the code below to plot those vectors, and try changing the numbers above.\n",
        "How can you make a very small angle between the vectors? How can you make a very large angle?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "QtbbBLgcFmE0"
      },
      "source": [
        "#@title Run this to create an interactive 3D plot\n",
        "#Code from https://stackoverflow.com/questions/47319238/python-plot-3d-vectors \n",
        "import numpy as np \n",
        "import plotly.graph_objs as go\n",
        "\n",
        "def vector_plot(tvects,is_vect=True,orig=[0,0,0]):\n",
        "    \"\"\"Plot vectors using plotly\"\"\"\n",
        "\n",
        "    if is_vect:\n",
        "        if not hasattr(orig[0],\"__iter__\"):\n",
        "            coords = [[orig,np.sum([orig,v],axis=0)] for v in tvects]\n",
        "        else:\n",
        "            coords = [[o,np.sum([o,v],axis=0)] for o,v in zip(orig,tvects)]\n",
        "    else:\n",
        "        coords = tvects\n",
        "\n",
        "    data = []\n",
        "    for i,c in enumerate(coords):\n",
        "        X1, Y1, Z1 = zip(c[0])\n",
        "        X2, Y2, Z2 = zip(c[1])\n",
        "        vector = go.Scatter3d(x = [X1[0],X2[0]],\n",
        "                              y = [Y1[0],Y2[0]],\n",
        "                              z = [Z1[0],Z2[0]],\n",
        "                              marker = dict(size = [0,5],\n",
        "                                            color = ['blue'],\n",
        "                                            line=dict(width=5,\n",
        "                                                      color='DarkSlateGrey')),\n",
        "                              name = 'Vector'+str(i+1))\n",
        "        data.append(vector)\n",
        "\n",
        "    layout = go.Layout(\n",
        "             margin = dict(l = 4,\n",
        "                           r = 4,\n",
        "                           b = 4,\n",
        "                           t = 4)\n",
        "                  )\n",
        "    fig = go.Figure(data=data,layout=layout)\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "vector_plot([v0,v1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7xDNfU3HUE3"
      },
      "source": [
        "For our Word2Vec vectors, we can imagine doing the same thing in 300-dimensional space. Of course, it's much harder to plot that! [Here](https://projector.tensorflow.org/) is one representation that you can play around with.\n",
        "\n",
        "Then we find the cosine of the angle between the two vectors to get the similarity. \n",
        "\n",
        "If the vectors are exactly the same, the angle will be 0, so we get a similarity of $cos(0) = 1$.\n",
        "\n",
        "If the vectors are exactly opposite, the angle will be 180 degrees, so we get a similarity of $cos(180) = -1$.\n",
        "\n",
        "There's a useful [mathematical trick](https://www.mathsisfun.com/algebra/vectors-dot-product.html) to find the cosine similarity:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/1d94e5903f7936d3c131e040ef2c51b473dd071d)\n",
        "\n",
        "Where $A_1, A_2, ..., A_{300}$ are the elements of the first vector and $B_1, B_2, ..., B_{300}$ are the elements of the second vector.\n",
        "\n",
        "Please implement cosine similarity below, and test it out using our 3-dimensional vectors from above. Do the results make sense?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAnA_KdoNqr-"
      },
      "source": [
        "def vector_cosine_similarity(vec1,vec2):\n",
        "  #YOUR CODE HERE\n",
        "  pass\n",
        "  #Return a number between -1 and 1\n",
        "\n",
        "print(vector_cosine_similarity(v0,v1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ2JQmZELItA"
      },
      "source": [
        "Now, use your cosine similarity function to calculate the similarity between two words. Try out a few words from the dataset - what pairs of words can you find that are particularly similar or particularly dissimilar?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwC6EioNJHKR"
      },
      "source": [
        "def word_similarity(word1, word2):\n",
        "  #Should return a similarity between -1 and 1\n",
        "  \n",
        "  try:\n",
        "    vec1 = vocab_dict[word1]\n",
        "    vec2 = vocab_dict[word2]\n",
        "\n",
        "    #TODO: Fill in the return statement here\n",
        "\n",
        "  except KeyError:\n",
        "    print ('Word not in dictionary')\n",
        "\n",
        "print(word_similarity('burger','steak'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8KKXG2BLz7K"
      },
      "source": [
        "Now, we can use our functions above to find the *most* similar words to any particular word. \n",
        "\n",
        "`find_most_similar(start_vec)` should output the top 5 words whose vectors are most similar to start_vec, with their similarities. Please fill it in.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgNpY_wZpG5A"
      },
      "source": [
        "def find_nearest_neighbor(word):\n",
        "  try:\n",
        "    vec = vocab_dict[word]\n",
        "    find_most_similar(vec)\n",
        "  except KeyError:\n",
        "    print ('Word not in dictionary')\n",
        "\n",
        "def find_most_similar(start_vec):\n",
        "  #Should print the top 5 most similar words to start_vec, and their similarities.,\n",
        "  #Hint: use a for loop to iterate through vocab_dict.\n",
        "  #Consider using a Pandas series.\n",
        "\n",
        "  #YOUR CODE HERE\n",
        "  pass\n",
        "  \n",
        "find_nearest_neighbor('bagel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxcyY1YZO9u5"
      },
      "source": [
        "Finally, we can use the functions we've built to complete word analogies, like the ones you can try out [here](http://bionlp-www.utu.fi/wv_demo/). For example:\n",
        "\n",
        "*   Breakfast is to bagel as lunch is to ________,\n",
        "\n",
        "This requires a bit of \"word arithmetic\":\n",
        "let's say A1, A2, and B1 are vectors for three words we know. We're trying to find B2 to complete \n",
        "\n",
        "*   A1 is to A2 as B1 is to B2.\n",
        "\n",
        "Intuitively, this means that the difference between A1 and A2 is the same as the difference between B1 and B2. So we write\n",
        "\n",
        "*   A1 - A2 = B1 - B2\n",
        "\n",
        "**Solve for B2:**\n",
        "\n",
        "*   B2 = ________________\n",
        "\n",
        "Once we know the vector that we \"expect\" for B2, we can use our previous functions to find the word whose representation is closest to that vector. Try it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfTeKGqYLw_1"
      },
      "source": [
        "def find_analogy(word_a1, word_a2, word_b1):\n",
        "  #Convert the words to vectors a1, a2, b1\n",
        "  #If word_a1:word_a2 as word_b1:word_b2, then \n",
        "  #a1 - a2 = b1 - b2\n",
        "  #So b2 = ...\n",
        "  #Calculate b2, and use your previous functions to find the best candidates for word_b2.\n",
        "\n",
        "  #YOUR CODE HERE\n",
        "  pass\n",
        "  \n",
        "find_analogy('breakfast','bagel','lunch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlDcBn9kQxNR"
      },
      "source": [
        "Word arithmetic doesn't always work perfectly - it's pretty tricky to find good examples! Which can you discover?\n",
        "\n",
        "If you're looking for a way to expand further on this exercise, you can try seeing what happens when you use [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance), another common measurement, instead of cosine similarity."
      ]
    }
  ]
}